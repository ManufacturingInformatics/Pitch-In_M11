""" Collection of functions used for reading, writing and searching the data generated and collected through BEAM thermal data simulations and runs to a
    predetermined file structure.

    Created by David Miller, 19/06/2019
    Modified by David Miller, 28/08/2019

    This file requires the following packages to be installed:
        * numpy
        * h5py
        
    The data is written to a HDF5 file in a hard-coded structure organised as follows:

    Runs
    |
    {1 .. inf}
    |
    =--Data
    |     |
    |     =- Laser
    |     |      |
    |     |      =- x
    |     |      |
    |     |      =- y
    |     |      |
    |     |      =- z
    |     |      |
    |     |      =- t
    |     |      |
    |     |      =- T
    |     |      |
    |     |      =- vel
    |     |
    |     =- Camera
    |            |
    |            =- Qr
    |            |
    |            =-Raspberry-Pi
    |               |
    |               =-pi-camera-data-*
    |            
    |
    =--Predictions
          |
          =- P
          |
          =- I
          |
          =- r0
          |
          =- T
          |
          =- Tp

    The purpose of each object are as follows:
        * Runs        : A collection of incrementally numbered groups, starting from 1, containing the data collected from a production run and the
                        data predicted using this data.
        * {1..inf}    : The data collected in a specific production run and the predictions made using that data and known material parameters.
                        The number indicates the order in which the run was created in the file.
        * Data        : The data collected during the specific production run. Designed to contain the thermal camera footage and the data contained
                        in the XML log file for the motor controlling the laser position. To be stored, the files need to be processed by the functions
                        readH264 and readXMLData functions.
        * Laser       : Collection of datasets for the data contained in the XML log file.
        * x           : Laser motor position on the x-axis relative to a preset origin point. Units are mm
        * y           : Laser motor position on the y-axis relative to a preset origin point. Units are mm
        * z           : Laser motor position on the z-axis relative to a preset origin point. Units are mm
        * T           : Laser motor torque. Units are N/m
        * vel         : Laser motor velocity. Units are mm/s
        * t           : Time at which the values are sampled relative to start of recording. Applies to all laser datasets as they are all sampled at the same rate.
        * Camera      : Group containing a dataset for the thermal camera data. It's a group to allow other thermal camera data to be stored
        * Qr          : The footage captured by the thermal camera as processed by the readH264 function. It is named as the camera captures radiative heat.
                        The image resolution is known to be 128x128
        * Raspberry-Pi: Group containing the datasets recorded by Raspberry Pi thermal camera
        * pi-camera-data-* : Dataset recorded by a Raspberry Pi. Not necessarily named pi-camera-data.
        * Predictions : Collection of datasets generated by the thermal model representing predicted laser paramters such as laser power and material behaviour
                        such as surface temperature. It's a group to allow other results to be added.
        * P           : Predicted laser power profile. Units are Watts
        * I           : Predicted laser power density. Units are Watts/m2
        * r0          : Predicted laser radius. Units are m
        * T           : Predicted surface temperature. Units are Kelvin
        * Tp          : Predicted peak surface temperature. Units are Kelvin

    NOTES
    ====================
        * It is the responsibility of the user to close the file when they're finished with it. Call close method the file object.
        * If the user wishes to use the context manager for the file object, they need to create the Runs group otherwise most of the functions will fail.
"""

import numpy as np
import h5py

def readH264(path,flag='mask'):
    """ Read thermal camera data and return the data arrat

        path : path to the file
        flag : string indicating any preprocessing that needs to be peformed with the data.
               Default : 'mask' : mask the data as per the camera processing documents
               Leave blank, '', if you want the raw data as float16

        Reads in the H264 camera data as a large vector, processes it as per the flag
        and reshapes the data into the known size of (128,128) and however many frames there are in the
        data.

        
        Returns the data as a numpy array of type float16
    """
    from numpy import fromfile,bitwise_and,split,reshape,dstack
    # known size of the images
    rows = 128
    cols = 128

    # read in raw bytes as a 1D array
    arr = fromfile(path,dtype='uint16')
    if arr.shape[0]==0:
        return 
    if flag=='mask':
        ## update values based on code
        # get code
        code_array = bitwise_and(arr,0xF000)
        # CODE_VAL_SEUIL2
        arr[code_array==0xD000] = 0xF800
        # CODE_VAL_CONTOUR
        arr[code_array==0xB000] = 0xF81F
        # CODE_VAL_MAX
        arr[code_array==0xC000] = 0x0000
        # CODE_VAL_SEUIL1
        arr[code_array==0xE000] = 0x001F

    ## just lower 12-bits
    arr = bitwise_and(arr,0x0FFF)

    ## convert data to frames
    # break the data into chunks that are 1d frames
    frames_set = split(arr,int(arr.shape[0]/(rows*cols)))
    # combined frames together into a 3d array
    return dstack([reshape(f,(rows,cols)) for f in frames_set])

def readXMLData(path):
    import xml.etree.ElementTree as ET
    """ Reads data from the given laser machine XML file

        =====ONLY READS FIRST DATA FRAME=====
        Typically only one frame in the file anyway

        path : file path to xml file

        Returns information as a list in the order
        header,time,torque,vel,x,y,z

        header : header information from the Data Frame in XML file
            time   : Sampling time vector for the values
            torque : Current applied to the motor to generate torque (A)
            vel    : motor velocity (mm/s)
            x      : motor position along x-axis (mm)
            y      : motor position along y-axis (mm)
            z      : motor position along z=axis (mm)
            
        time,torque,vel,x,y,z are lists of data

        header is a dictionary with the following entries:
            - "Date-Time" : Datetime stamp of the first Data Frame
            
            - "data-signal-0" : Dictionary of information about torque
                with the following items:
                + "interval" : time between samples
                + "description" : description of the signal
                + "count" : number of data points
                + "unitType" : description of what the data represents
                
            - "data-signal-1" : Dictionary of information about velocity
                with the following items:
                + "interval" : time between samples
                + "description" : description of the signal
                + "count" : number of data points
                + "unitType" : description of what the data represents
                
            - "data-signal-2" : Dictionary of information about x
                with the following items:
                + "interval" : time between samples
                + "description" : description of the signal
                + "count" : number of data points
                + "unitType" : description of what the data represents
                
            - "data-signal-3" : Dictionary of information about y
                with the following items:
                + "interval" : time between samples
                + "description" : description of the signal
                + "count" : number of data points
                + "unitType" : description of what the data represents
                
            - "data-signal-4" : Dictionary of information about z
                with the following items:
                + "interval" : time between samples
                + "description" : description of the signal
                + "count" : number of data points
                + "unitType" : description of what the data represents
    """
    # parse xml file to a tree structure
    tree = ET.parse(path)
    # get the root/ beginning of the tree
    #root  = tree.getroot()
    # get all the trace data
    log = tree.findall("traceData")
    # get the number of data frames/sets of recordings
    log_length = len(log)

    # data sets to record
    x = []
    y = []
    z = []
    torque = []
    vel = []
    time = []
    header = {}
    
    ## read in log data
    # for each traceData in log
    for f in log:
        # only getting first data frame as it is known that there is only one frame
        # get the header data for data frame
        head = f[0].findall("frameHeader")

        # get date-timestamp for file
        ### SYNTAX NEEDS TO BE MODIFIED ###
        temp = head[0].find("startTime")
        if temp == None:
            header["Date-Time"] = "Unknown"
        else:
            header["Date-Time"] = temp
            
        # get information about data signals
        head = f[0].findall("dataSignal")
        # iterate through each one
        for hi,h in enumerate(head):
            # create entry for each data signal recorded
            # order arranged to line up with non-time data signals
            header["data-signal-{:d}".format(hi)] = {"interval": h.get("interval"),
                                                     "description": h.get("description"),
                                                     "count":h.get("datapointCount"),
                                                     "unitType":h.get("unitsType")}
            # update unitType to something meaningful                            
            if header["data-signal-{:d}".format(hi)]["unitType"] == 'posn':
                header["data-signal-{:d}".format(hi)]["unitType"] = 'millimetres'
            elif header["data-signal-{:d}".format(hi)]["unitType"] == 'velo':
                header["data-signal-{:d}".format(hi)]["unitType"] = 'mm/s'
            elif header["data-signal-{:d}".format(hi)]["unitType"] == 'current':
                header["data-signal-{:d}".format(hi)]["unitType"] = 'Amps'
                                
        # get all recordings in data frame
        rec = f[0].findall("rec")
        # parse data and separate it into respective lists
        for ri,r in enumerate(rec):
            # get timestamp value
            time.append(float(r.get("time")))
            
            # get torque value
            f1 = r.get("f1")
            # if there is no torque value, then it hasn't changed
            if f1==None:
                # if there is a previous value, use it
                if ri>0:
                    torque.append(float(torque[ri-1]))
            else:
                torque.append(float(f1))

            # get vel value
            f2 = r.get("f2")
            # if there is no torque value, then it hasn't changed
            if f2==None:
                # if there is a previous value, use it
                if ri>0:
                    vel.append(float(vel[ri-1]))
            else:
                vel.append(float(f2))

            # get pos1 value
            val = r.get("f3")
            # if there is no torque value, then it hasn't changed
            if val==None:
                # if there is a previous value, use it
                if ri>0:
                    x.append(float(x[ri-1]))
            else:
                x.append(float(val))

            # get pos2 value
            val = r.get("f4")
            # if there is no torque value, then it hasn't changed
            if val==None:
                # if there is a previous value, use it
                if ri>0:
                    y.append(float(y[ri-1]))
            else:
                y.append(float(val))

            # get pos3 value
            val = r.get("f5")
            # if there is no torque value, then it hasn't changed
            if val==None:
                # if there is a previous value, use it
                if ri>0:
                    z.append(float(z[ri-1]))
            else:
                z.append(float(val))

    return header,time,torque,vel,x,y,z

def readXMLDataDict(path):
    import xml.etree.ElementTree as ET
    """ Reads data from the given laser machine XML file

        =====ONLY READS FIRST DATA FRAME=====
        Typically only one frame in the file anyway

        path : file path to xml file

        Returns the information as a dictionary with the keywords:
            header : header information from the Data Frame in XML file
            time   : Sampling time vector for the values
            torque : Current applied to the motor to generate torque (A)
            vel    : motor velocity (mm/s)
            x      : motor position along x-axis (mm)
            y      : motor position along y-axis (mm)
            z      : motor position along z=axis (mm)

        time,torque,vel,x,y,z are lists of data

        header is a dictionary with the following entries:
            - "Date-Time" : Datetime stamp of the first Data Frame
            
            - "data-signal-0" : Dictionary of information about torque
                with the following items:
                + "interval" : time between samples
                + "description" : description of the signal
                + "count" : number of data points
                + "unitType" : description of what the data represents
                
            - "data-signal-1" : Dictionary of information about velocity
                with the following items:
                + "interval" : time between samples
                + "description" : description of the signal
                + "count" : number of data points
                + "unitType" : description of what the data represents
                
            - "data-signal-2" : Dictionary of information about x
                with the following items:
                + "interval" : time between samples
                + "description" : description of the signal
                + "count" : number of data points
                + "unitType" : description of what the data represents
                
            - "data-signal-3" : Dictionary of information about y
                with the following items:
                + "interval" : time between samples
                + "description" : description of the signal
                + "count" : number of data points
                + "unitType" : description of what the data represents
                
            - "data-signal-0" : Dictionary of information about z
                with the following items:
                + "interval" : time between samples
                + "description" : description of the signal
                + "count" : number of data points
                + "unitType" : description of what the data represents
    """
    # parse xml file to a tree structure
    tree = ET.parse(path)
    # get the root/ beginning of the tree
    #root  = tree.getroot()
    # get all the trace data
    log = tree.findall("traceData")
    # get the number of data frames/sets of recordings
    log_length = len(log)

    # data sets to record
    x = []
    y = []
    z = []
    torque = []
    vel = []
    time = []
    header = {}
    
    ## read in log data
    # for each traceData in log
    for f in log:
        # only getting first data frame as it is known that there is only one frame
        # get the header data for data frame
        head = f[0].findall("frameHeader")

        # get date-timestamp for file
        ### SYNTAX NEEDS TO BE MODIFIED ###
        temp = head[0].find("startTime")
        if temp == None:
            header["Date-Time"] = "Unknown"
        else:
            header["Date-Time"] = temp
            
        # get information about data signals
        head = f[0].findall("dataSignal")
        # iterate through each one
        for hi,h in enumerate(head):
            # create entry for each data signal recorded
            # order arranged to line up with non-time data signals
            header["data-signal-{:d}".format(hi)] = {"interval": h.get("interval"),
                                                     "description": h.get("description"),
                                                     "count":h.get("datapointCount"),
                                                     "unitType":h.get("unitsType")}
            # update unitType to something meaningful                            
            if header["data-signal-{:d}".format(hi)]["unitType"] == 'posn':
                header["data-signal-{:d}".format(hi)]["unitType"] = 'millimetres'
            elif header["data-signal-{:d}".format(hi)]["unitType"] == 'velo':
                header["data-signal-{:d}".format(hi)]["unitType"] = 'mm/s'
            elif header["data-signal-{:d}".format(hi)]["unitType"] == 'current':
                header["data-signal-{:d}".format(hi)]["unitType"] = 'Amps'
                                
        # get all recordings in data frame
        rec = f[0].findall("rec")
        # parse data and separate it into respective lists
        for ri,r in enumerate(rec):
            # get timestamp value
            time.append(float(r.get("time")))
            
            # get torque value
            f1 = r.get("f1")
            # if there is no torque value, then it hasn't changed
            if f1==None:
                # if there is a previous value, use it
                if ri>0:
                    torque.append(float(torque[ri-1]))
            else:
                torque.append(float(f1))

            # get vel value
            f2 = r.get("f2")
            # if there is no torque value, then it hasn't changed
            if f2==None:
                # if there is a previous value, use it
                if ri>0:
                    vel.append(float(vel[ri-1]))
            else:
                vel.append(float(f2))

            # get pos1 value
            val = r.get("f3")
            # if there is no torque value, then it hasn't changed
            if val==None:
                # if there is a previous value, use it
                if ri>0:
                    x.append(float(x[ri-1]))
            else:
                x.append(float(val))

            # get pos2 value
            val = r.get("f4")
            # if there is no torque value, then it hasn't changed
            if val==None:
                # if there is a previous value, use it
                if ri>0:
                    y.append(float(y[ri-1]))
            else:
                y.append(float(val))

            # get pos3 value
            val = r.get("f5")
            # if there is no torque value, then it hasn't changed
            if val==None:
                # if there is a previous value, use it
                if ri>0:
                    z.append(float(z[ri-1]))
            else:
                z.append(float(val))

    # construct and return items as a dictionaru
    return {"header":header,"time":time,"torque":torque,"vel":vel,"x":x,"y":y,"z":z}

def createRunGroup(h5_file,material="SS-316L"):
    """ Create next incrementally named run group to file

        h5_file : h5py file object

        Returns None if failed to create a new entry in Run group
        or Run group does not exist

        Reasons for failure:
            - No Runs super group
    """
    import datetime
    # check if data node exists
    # if not exit early
    if not "Runs" in h5_file:
        print("No runs super-group in file")
        return None
    else:
        # get Data group of file
        runs= h5_file["Runs"]
        # get the current max number of characters in run name
        # as set in attribute "Run Max Length". Returns -1 if not created
        max_runn = runs.attrs.get("Run Max Length",default=-1)
        # if the attribute doesn't exist, return False
        # get current max run number 
        #curr_max = runs.visititems(findMaxRunNum)
        curr_max = getNumRuns(h5_file)
        # if max is -1, then no runs have been added to Data group
        # if curr found max is 
        if ((curr_max == 0) and (max_runn == -1)):
            new_run = runs.create_group("1")
            runs.attrs["Run Max Length"] = 1 # create attribute for max length of run name
        else:
            # create new key name
            new_key = str(curr_max+1)
            #print(curr_max,new_key)
            # create new run group
            new_run = runs.create_group(new_key)
            runs.attrs["Run Max Length"] += 1

        # update attributes
        new_run.attrs["Material"] = material
        new_run.attrs["Date-Time"] = datetime.datetime.now().isoformat()

        # create sub groups for data runs
        data = new_run.create_group("Data")
        data.create_group("Laser")
        data.create_group("Camera")

        # create group for predictions    
        new_run.create_group("Predictions")
        # return new run group object
        return new_run

def getRun(h5_file,run_num):
    """ Retrieve the specified run group from the file

        h5_file : File object
        run_num : The requested non-zero run number.
                  If run_num is -1, then all runs are returned
                  as a list

        Returns None if:
            - Run does not exist
            - Runs super-group does not exist
            - Specfied run number is less than eq. 0 and not -1
    """
    # if Runs super-group does not exist, return None
    if not "Runs" in h5_file:
        return None
    # if run_num is less than or eq. 0, return None
    # File is structured such that the runs are numbered from 1 onwards
    elif (run_num <=0) and (run_num != -1):
        return None
    # if run_num is -1 return all runs under Runs super-group
    elif run_num == -1:
        # create empty list
        allRuns = []
        # iterate through possible run names
        for i in range(getNumRuns(h5_file)):
            # check if run exists before adding to list
            # just in case user has modified or removed runs
            if "Runs/{:d}".format(i+1) in h5_file:
                allRuns.append(h5_file["Runs/{:d}".format(i+1)])
    else:
        # if the requested run does not exist, return None
        if not "Runs/{:d}".format(run_num) in h5_file:
            return None
        # else return requested run group
        else:
            return h5_file["Runs/{:d}".format(run_num)]

def getLatestRun(h5_file):
    """ Find the latest run in file

        h5_file : File object
        
        Returns None if:
            - No Runs super-group in file
            - No Runs in group
    """
    if not "Runs" in h5_file:
        return None
    else:
        data_group = h5_file["Runs"]
        if data_group.attrs.get("Run Max Length",default=-1) == -1:
            return None
        else:
            #curr_max = data_group.visit(findMaxRunNum)
            curr_max = getNumRuns(h5_file)
            if curr_max==-1:
                return None
            else:
                return data_group["{:d}".format(curr_max)]

def addQr(h5_file,Qr,createNew=False,material="SS-316L"):
    """ Just add thermal camera data to the file

        h5_file : H5Py file object
        Qr : thermal camera data, numpy array
        createNew : create new run group to store data in. Default False
        material : Material associated with the data. Used if creating new run.

        Adds the radiative thermal camera data to the HDF5 file. If a path is given,
        the readH264 method is called to read and process the data. Updates run
        date time stamp
        
        Returns False if unsuccessfull and returns True, dataset object if successful
    """
    # if there's no Runs group in data, then initialize has not been called yet
    # return False
    if not "Runs" in h5_file:
        return False,None
    else:
        # if createNew flag has been set, call fn to create new run group
        if createNew:
            run_group = createRunGroup(h5_file,material)
        # if createNew flag is false, get latest fun
        else:
            run_group = getLatestRun(h5_file)

        # if either method has failed, return false
        if not run_group:
            return False,None
                    
        # create dataset using the given data
        if not "Data/Camera/Qr" in run_group:
            qr_dataset = run_group["Data/Camera"].create_dataset("Qr",data=Qr)
            qr_dataset.attrs["description"] = "Radiative Heat values recorded by thermal camera"
            qr_dataset.attrs["unitType"] = "Joules/m2"
        else:
            qr_dataset = run_group["Data/Camera/Qr"]
            qr_dataset[...] = Qr
            
        # add attributes
        qr_dataset.attrs["size"] = qr_dataset.shape
        qr_dataset.attrs["dtype"] = str(qr_dataset.dtype)
        

        # update date time attribute
        run_group.attrs["Date-Time"] = datetime.datetime.now().isoformat()

        # return True and dataset object
        return True,qr_dataset

def addXML(h5_file,xml_data,createNew=False,material="SS-316L"):
    """ Just add XML data to the file

        h5_file :H5Py file object
        xml_data : list of xml data as returned by readXMLData
        createNew : Flag to create new run group to store data in. Default False means
                    the latest run is used.
        material : Material associated with the data. Used if creating new run.

        Adds the XML log data to the HDF5 file. The createNew flag allows the creation
        of a new run group to accomodate the data. Updates the "Date-Time" attribute
        of the updated run group after the data has been modified
        
        Returns False if unsuccessfull and returns True, new run group if successful
    """
    import datetime
    # checks to see if the file has been initialized
    # if not return false
    if not "Runs" in h5_file:
        return False,None
    else:
        # if createNew flag has been set, call fn to create new run group
        if createNew:
            run_group = createRunGroup(h5_file,material)
        # if createNew flag is false, get latest fun
        else:
            run_group = getLatestRun(h5_file)

        # if either method has failed, return false
        if not run_group:
            return False,None
         
        ## xml data
        # add DT timestamp from the header file
        run_group["Data/Laser"].attrs["Log-Date-Time"] = xml[0]["Date-Time"]
        
        # add time dataset
        if not "Data/Laser/t" in run_group:
            dataset = run_group["Data/Laser"].create_dataset("t",data=xml[1])
        else:
            dataset = run_group["Data/Laser/t"]
            dataset[...] = xml[1]
            
        dataset.attrs["size"] = dataset.shape
        dataset.attrs["dtype"] = str(dataset.dtype)
        dataset.attrs["description"] = "Sample time"
        dataset.attrs["unitType"] = "seconds"

        # order the values appear in the xml data if supplied by readXML function
        data_order = ['T','vel','x','y','z']
        # iterate through each value in data order also getting idx
        for di,d in enumerate(data_order):
            # create dataset path
            dpath = "Data/Laser/{:s}".format(d)
            # if dataset does not exist, create it using the data in xml
            # xml data is formatted as [header,t,T,vel,x,y,z]
            if not dpath in run_group:
                dataset = run_group["Data/Laser"].create_dataset(d,data=xml[di+2])
            # else get existing dataset
            else:
                dataset = run_group[dpath]
                dataset[...] = xml[di+2]

            # iterate through header information associated with the data signal
            # header information is ordered the same way as data order so
            # data-signal-0 corresponds to T, data-signal-1 corresponds to vel etc.
            for key,item in xml[0]["data-signal-{:d}".format(di)].items():
                dataset.attrs[key] = item
                
            # create/update size and datatype attributes
            dataset.attrs["size"] = dataset.shape
            dataset.attrs["dtype"] = str(dataset.dtype)

        # update date time attribute
        run_group.attrs["Date-Time"] = datetime.datetime.now().isoformat()

        # return success and run group
        return True,run_group
        
def addRaspPiCamera(h5_file,pi_data,createNew=False,material="SS-316L"):
    """ Just add XML data to the file

        h5_file : H5Py file
        pi_data : Collection of Raspberry pi camera data
        createNew : Flag to create new run group to store data in. Default False means
                    the latest run is used.
        material : Material associated with the data. Used if creating new run.

        Adds the Raspberry Pi camera data to the HDF5 file. The createNew flag allows the creation
        of a new run group to accomodate the data. Updates the "Date-Time" attribute
        of the updated run group after the data has been modified
        
        Returns False if unsuccessfull and returns True, new run group if successful
    """
    import datetime
    # checks to see if the file has been initialized
    # if not return false
    if not "Runs" in h5_file:
        return False,None
    else:
        # if createNew flag has been set, call fn to create new run group
        if createNew:
            run_group = createRunGroup(h5_file,material)
        # if createNew flag is false, get latest fun
        else:
            run_group = getLatestRun(h5_file)

        # if either method has failed, return false
        if not run_group:
            return False,None
        
        # check if the raspberry pi group exists
        # if it doesn't, create it
        if not "raspberry-pi" in run_group["Data/Camera"]:
            pi_group = run_group["Data/Camera"].create_group("Raspberry-Pi")
        # if it's a single dataset
        if type(pi_data) is np.ndarray:
            if not "pi-camera-data-1" in pi_group:
                # create a single dataset called pi-camera-data-1
                dataset = pi_group.create_dataset("pi-camera-data-1",data=pi_data)
                dataset.attrs["description"] = "Radiative Heat values recorded by a Raspberry Pi Thermal Camera"
                dataset.attrs["unitType"] = "Joules/m2"
            else:
                dataset = pi_group["pi-camera-data-1"]
                dataset[...] = pi_data
                
             dataset.attrs["size"] = dataset.shape
             dataset.attrs["dtype"] = str(dataset.dtype)
        # if it's a list of datasets
        elif type(pi_data) is list:
            # iterate through list naming datasets incremenetally
            for cd,d in enumerate(pi_data):
                if not "pi-camera-data-{0}".format(cd) in pi_group:
                    dataset = pi_group.create_dataset("pi-camera-data-{0}".format(cd),data=d)
                    dataset.attrs["description"] = "Radiative Heat values recorded by a Raspberry Pi Thermal Camera"
                    dataset.attrs["unitType"] = "Joules/m2"
                else:
                    dataset = pi_group["pi-camera-data-{0}"]
                    dataset[...] = d
                
                dataset.attrs["size"] = dataset.shape
                dataset.attrs["dtype"] = str(dataset.dtype)
        # if it's a dictionary of datasets
        elif type(pi_data) is dict:
            # iterate through dictionary creating datasets based off the keys
            for key,val in pi_data.items():
                if not key in pi_group:
                    dataset = pi_group.create_dataset(key,data=val)
                    dataset.attrs["description"] = "Radiative Heat values recorded by a Raspberry Pi Thermal Camera"
                    dataset.attrs["unitType"] = "Joules/m2"
                else:
                    dataset = pi_group[key]
                    dataset[...] = val
                    
                dataset.attrs["size"] = dataset.shape
                dataset.attrs["dtype"] = str(dataset.dtype)
        # update time stamp
        run_group.attrs["Date-Time"] = datetime.datetime.now().isoformat()
        
def getSetsFromDirList(path):
    """ Iterate through target folder and collect dataset to return as a list
    
        path : Absolute path to folder containing HDf5 files
        
        Iterates through found HDF5 files and uses getAllDatasets to collect all 
        the datasets in the file. The datasets are added to a list that is 
        returned at the end.
        
        Does NOT go through nested directories
        
        If the path is not a directory, an empty list is returned.
        
        Returns a list of data sets found in the target folder.
    """
    import os
    # if the path is not to a dictionary, return empty list
    if not os.path.isdir(path):
        return []
    # form empty list
    data = []
    # convert path to directory object
    dir = os.fsencode(path)
    # iterate through files in directory
    for fn in os.listdir(dir):
        # decode the filename to string
        filename = os.fsdecode(fn)
        # if the file name ends with .hdf5
        if filename.endswith(".hdf5"):
            # attempt to open HDf5 in read mode
            h5_file = h5py.File(filename,'r')
            # call function to get all datasets in file
            datasets = getAllDatasets(h5_file)
            # extend list with found datasets
            data.extend(list(datasets.values()))
            h5_file.close()
    return data
    
def getSetsFromDirDict(path):
    """ Iterate through target folder and collect dataset to return as a directory
    
        path : Absolute path to folder containing HDf5 files
        
        Iterates through found HDF5 files and uses getAllDatasets to collect all 
        the datasets in the file. The datasets are added to a dictionary which
        is keyworded by dataset name.
        
        If the dataset names are used as the key, if multiple files have datasets
        of the same name, it will repeatedly override the same data.
        
        Does NOT go through nested directories
        
        If the path is not a directory, an empty list is returned.
        
        Returns a directory of data sets found in the target folder.
    """
    import os
    # if the path is not to a dictionary, return empty directory
    if not os.path.isdir(path):
        return {}
    # form empty directory
    data = {}
    # convert path to directory object
    dir = os.fsencode(path)
    # iterate through files in directory
    for fn in os.listdir(dir):
        # decode the filename to string
        filename = os.fsdecode(fn)
        # if the file name ends with .hdf5
        if filename.endswith(".hdf5"):
            # attempt to open HDf5 in read mode
            h5_file = h5py.File(filename,'r')
            # call function to get all datasets in file
            datasets = getAllDatasets(h5_file)
            # update dictionary with found datasets
            data.update(datasets)
            h5_file.close()
    return data

def getAllDatasets(obj):
    """ Get all dataset objects in file

        obj : h5py object such as file or group

        Iterates through obj to find all datasets building 
        a dictionary where the keys are the datasets names
        
        Returns a dictionary of dataset objects found in obj
    """
    import h5py
    # create empty list
    dsets = {}
    # if obj is a dataset, append to list
    if isinstance(obj,h5py.Dataset):
        dsets[obj.name]=obj[()]
    # if the obj is a group or file, iterate through it
    elif type(obj) in [h5py._hl.group.Group,h5py._hl.files.File]:
        # for each obj in given obj, recusrive call fn
        for v in obj.values():
            next_sets = getAllDatasets(v)
            # if something was returned, add returned list to dsets
            if len(next_sets)>0:
                dsets.update(next_sets)
    # return results
    return dsets
                            
def getDatasets(obj,*args):
    """ Get the specified datasets from the h5py object

        obj : h5py object (e.g. File, Group or Dataset)
        *args : Requested datasets
        
        Valid keywords:
            xml : XML data from readXML fn
            Qr : thermal camera data
            P : Predicted laser power
            r0 : predicted laser radius
            T : surface temperature
            Tp : peak temperature
            I : surface laser power density matrix
            
        Returns a dictionary of of the requested datasets
    """
    import h5py
    # create empty list
    dsets = {}
    # if obj is a dataset, append to list
    if isinstance(obj,h5py.Dataset):
        # get the characters after the final slash in name
        # this is the actual name of the data set
        # check to see if it is in the requested datasets supplied in *args
        # if it is, add to list
        if obj.name[obj.name.rfind('/')+1] in args:
            dsets[obj.name]=obj[()]
    # if the obj is a group or file, iterate through it
    elif type(obj) in [h5py._hl.group.Group,h5py._hl.files.File]:
        # for each obj in given obj, recusrive call fn
        for v in obj.values():
            next_sets = getDatasets(v)
            # if something was returned, add returned list to dsets
            # extend adds each value of the list to the list as opposed to simply
            # inserting the list inside the list
            if len(next_sets)>0:
                dsets.update(next_sets)
    # return results
    return dsets

def getDatasetsFrom(h5_file,run_num,*args):
    """ Get the specified datasets from the specified run in file

        h5_file: h5py file object
        run_num : specified non-zero numbered run
        *args : Requested datasets

        Wrapper function for calling getDatasets. Generates the necessary
        tree path internally and calls getDatasets with it as the object
        
        Valid keywords:
            xml : XML data from readXML fn
            Qr : thermal camera data
            P : Predicted laser power
            r0 : predicted laser radius
            T : surface temperature
            Tp : peak temperature
            I : surface laser power density matrix
    """
    # check if the specified run exists
    # saves jumping through the tree with getDatasets
    if not "Runs/{:d}".format(run_num) in h5_file:
        return []
    else:
        # call getDatasets passing the specified run group
        return getDataset(h5_file["Runs/{:d}".format(run_num)],*args)
            
def updateData(h5_file,createNew=False,material="SS-316L",**kwargs):
    """ Add data to the file

        h5_file : h5py file object
        createNew : flag to create new run group. If False then the most recent run is
                    used.
        material : Material used in run
        *kwargs : Data to be added to be added to the run group

        Valid keywords:
            run_num : Specified run to update. Overridden if createNew is set.
            xml : XML data structure from readXML fn
            x : laser x position data
            y : laser y position data
            z : laser z position data
            t : laser sample time vector
            vel : laser velocity
            T : laser motor torque
            Qr : thermal camera data
            P : Predicted laser power
            r0 : predicted laser radius
            T : surface temperature
            Tp : peak temperature
            I : surface laser power density matrix
            pi : All thermal camera data recorded by a set of Raspberry Pis
            
        The Raspberry Pi data can be supplied in three forms and each are processed
        differently:
            - single dataset : Single dataset recorded by Pi. It will be stored as pi-camera-data-1.
            - list of datasets : Multiple datasets recorded from several Pis stored in a list.
                                 Datasets are named incrementally based on position e.g. pi-camera-data-1,
                                 pi-camera-data-2 etc.
            - dictionary of datasets : Keyword datasets recorded by several Pis. The names of the datasets
                                       are set as the keys of the dataset. This gives the user control over
                                       what they are called in case the names matter

        Updates the Date-Time attribute of the run group as well.
        If the user is only updating/creating any of the laser data
        (x,y,z,t,T,vel), the attributes normally supplied by the header information
        in the xml file are NOT updated/created.
        
        Returns False,None if failed or True,updated run group if successful
    """
    import datetime
    # check if there's a Runs super-group in file
    # if not then it hasn't been initialized
    if not "Runs" in h5_file:
        return False,None
    else:
        # if createNew flag has been set, call fn to create new run group
        if createNew:
            run_group = createRunGroup(h5_file)
        # if a specific run is named, collect it if it exists
        elif 'run_num' in kwargs:
            # check to see if the specified run exists
            if "Runs/{:d}".format(kwargs('run_num')) in h5_file:
                run_group = h5_file["Runs/{:d}".format(kwargs('run_num'))]
            else:
                run_group = None
        # if createNew flag is false, get latest fun
        else:
            run_group = getLatestRun(h5_file)

        # if either method has failed, return false
        if run_group == None:
            return False,None

        ## predictions data
        # power
        if 'P' in kwargs:
            # if the dataset does not exist in the run group already,
            # create it using the given data
            if not "Predictions/P" in run_group:
                dataset = run_group["Predictions"].create_dataset("P",data=kwargs['P'])
                dataset.attrs["description"] = "Estimated laser power"
                dataset.attrs["unitType"] = "Watts"
            # if it does exist, update the dataset
            else:
                dataset = run_group["Predictions/P"]
                dataset[...] = kwargs['P']
            # update dataset meta data
            dataset.attrs["size"] = kwargs('P').shape
            dataset.attrs["dtype"] = str(kwargs('P').dtype)
            
        # power density
        if 'I' in kwargs:
            # if the dataset does not exist in the run group already,
            # create it using the given data
            if not "Predictions/I" in run_group:
                dataset = run_group["Predictions"].create_dataset("I",data=kwargs['I'])
                dataset.attrs["size"] = kwargs['I'].shape
                dataset.attrs["dtype"] = str(kwargs['I'].dtype)
            # if it does exist, update data
            else:
                dataset = run_group["Predictions/I"]
                dataset[...] = kwargs['P']
            # update meta information
            dataset.attrs["description"] = "Estimated laser power density"
            dataset.attrs["unitType"] = "Watts/m2"
        # laser radius
        if 'r0' in kwargs:
            if not "Predictions/r0" in run_group:
                dataset = run_group["Predictions"].create_dataset("r0",data=kwargs['r0'])
                dataset.attrs["description"] = "Estimated laser radius"
                dataset.attrs["unitType"] = "Metres"
            else:
                dataset = run_group["Predictions/r0"]
                dataset[...]=kwargs['r0']
                
            dataset.attrs["size"] = kwargs['r0'].shape
            dataset.attrs["dtype"] = str(kwargs['r0'].dtype)
            
        # surface temperature
        if 'T' in kwargs:
            if not "Predictions/T" in run_group:
                dataset = run_group["Predictions"].create_dataset("T",data=kwargs['T'])
                dataset.attrs["description"] = "Estimated surface temperature"
                dataset.attrs["unitType"] = "Kelvin"
            else:
                dataset = run_group["Predictions/T"]
                dataset[...] = kwargs['T']
                
            dataset.attrs["size"] = kwargs['T'].shape
            dataset.attrs["dtype"] = str(kwargs['T'].dtype)
            
        # peak surface temperature
        if 'Tp' in kwargs:
            if not "Predictions/Tp" in run_group:
                dataset = run_group["Predictions"].create_dataset("Tp",data=kwargs['Tp'])
                dataset.attrs["description"] = "Estimated peak temperature"
                dataset.attrs["unitType"] = "Kelvin"
            else:
                dataset = run_group["Predictions/Tp"]
                dataset[...] = kwargs['Tp']
                
            dataset.attrs["size"] = kwargs['Tp'].shape
            dataset.attrs["dtype"] = str(kwargs['Tp'].dtype)
            
        ## data sources
        # xml data
        if 'xml' in kwargs:
            # get given data
            xml = kwargs['xml']
            
            ## xml data
            # add DT timestamp from the header file
            run_group["Data/Laser"].attrs["Log-Date-Time"] = xml[0]["Date-Time"]
            # add time dataset
            if not "Data/Laser/t" in run_group:
                dataset = run_group["Data/Laser"].create_dataset("t",data=xml[1])
                dataset.attrs["description"] = "Sample time"
                dataset.attrs["unitType"] = "seconds"
            else:
                dataset = run_group["Data/Laser/t"]
                dataset[...] = xml[1]
                
            dataset.attrs["size"] = dataset.shape
            dataset.attrs["dtype"] = str(dataset.dtype)
            
            # order the values appear in the xml data if supplied by readXML function
            data_order = ['T','vel','x','y','z']
            # iterate through each value in data order also getting idx
            for di,d in enumerate(data_order):
                # create dataset path
                dpath = "Data/Laser/{:s}".format(d)
                # if dataset does not exist, create it using the data in xml
                # xml data is formatted as [header,t,T,vel,x,y,z]
                if not dpath in run_group:
                    dataset = run_group["Data/Laser"].create_dataset(d,data=xml[di+2])
                # else get existing dataset
                else:
                    dataset = run_group[dpath]
                    dataset[...] = xml[di+2]

                # iterate through header information associated with the data signal
                # header information is ordered the same way as data order so
                # data-signal-0 corresponds to T, data-signal-1 corresponds to vel etc.
                for key,item in xml[0]["data-signal-{:d}".format(di)].items():
                    dataset.attrs[key] = item
                    
                # create/update size and datatype attributes
                dataset.attrs["size"] = dataset.shape
                dataset.attrs["dtype"] = str(dataset.dtype)
                
        # just x pos
        if 'x' in kwargs:
            if not "Data/Laser/x" in run_group:
                dataset = run_group["Data/Laser"].create_dataset("x",data=kwargs['x'])
                dataset.attrs["description"] = "Laser Head Position on x-axis"
                dataset.attrs["unitType"] = "Millimetres"
            else:
                dataset = run_group["Data/Laser/x"]
                dataset[...] = kwargs['x']

            # only create/update the size and dtype attributes
            # the other ones normally supplied by the xml header are not modified
            dataset.attrs["size"] = dataset.shape
            dataset.attrs["dtype"] = str(dataset.dtype)

        # just y pos
        if 'y' in kwargs:
            if not "Data/Laser/y" in run_group:
                dataset = run_group["Data/Laser"].create_dataset("y",data=kwargs['y'])
                dataset.attrs["description"] = "Laser Head Position on y-axis"
                dataset.attrs["unitType"] = "Millimetres"
            else:
                dataset = run_group["Data/Laser/y"]
                dataset[...] = kwargs['y']
                
            dataset.attrs["size"] = dataset.shape
            dataset.attrs["dtype"] = str(dataset.dtype)

        # just z pos
        if 'z' in kwargs:
            if not "Data/Laser/z" in run_group:
                dataset = run_group["Data/Laser"].create_dataset("z",data=kwargs['z'])
                dataset.attrs["description"] = "Laser Head Position on z-axis"
                dataset.attrs["unitType"] = "Millimetres"
            else:
                dataset = run_group["Data/Laser/z"]
                dataset[...] = kwargs['z']
                
            dataset.attrs["size"] = dataset.shape
            dataset.attrs["dtype"] = str(dataset.dtype)

        # just time
        if 't' in kwargs:
            if not "Data/Laser/t" in run_group:
                dataset = run_group["Data/Laser"].create_dataset("t",data=kwargs['t'])
                dataset.attrs["description"] = "Sample time"
                dataset.attrs["unitType"] = "seconds" 
            else:
                dataset = run_group["Data/Laser/t"]
                dataset[...] = kwargs['t']
                
            dataset.attrs["size"] = dataset.shape
            dataset.attrs["dtype"] = str(dataset.dtype)
            
        # just torque
        if 'T' in kwargs:
            if not "Data/Laser/T" in run_group:
                dataset = run_group["Data/Laser"].create_dataset("T",data=kwargs['T'])
                dataset.attrs["description"] = "Current supplied to generate Torque"
                dataset.attrs["unitType"] = "Amps"
            else:
                dataset = run_group["Data/Laser/T"]
                dataset[...] = kwargs['T']
                
            dataset.attrs["size"] = dataset.shape
            dataset.attrs["dtype"] = str(dataset.dtype)

        # just velocity
        if 'vel' in kwargs:
            if not "Data/Laser/vel" in run_group:
                dataset = run_group["Data/Laser"].create_dataset("vel",data=kwargs['vel'])
                dataset.attrs["description"] = "Linear velocity of the motor"
                dataset.attrs["unitType"] = "Millimetres per seconds"
            else:
                dataset = run_group["Data/Laser/vel"]
                dataset[...] = kwargs['vel']
                
            dataset.attrs["size"] = dataset.shape
            dataset.attrs["dtype"] = str(dataset.dtype)

        # thermal camera data
        if 'Qr' in kwargs:
            if not "Data/Laser/Qr" in run_group:
                dataset = run_group["Data/Camera"].create_dataset("Qr",data=kwargs['Qr'])
                dataset.attrs["description"] = "Radiative Heat values recorded by thermal camera"
                dataset.attrs["unitType"] = "Joules/m2"
            else:
                dataset = run_group["Data/Camera/Qr"]
                dataset[...] = kwargs['Qr']
                
            dataset.attrs["size"] = dataset.shape
            dataset.attrs["dtype"] = str(dataset.dtype)
            
        # Raspberry pi camera data
        if 'pi' in kwargs:
            # extract data
            data = kwargs['pi']
            # check if the raspberry pi group exists
            # if it doesn't, create it
            if not "raspberry-pi" in run_group["Data/Camera"]:
                pi_group = run_group["Data/Camera"].create_group("Raspberry-Pi")
            # if it's a single dataset
            if type(data) is np.ndarray:
                if not "pi-camera-data-1" in pi_group:
                    # create a single dataset called pi-camera-data-1
                    dataset = pi_group.create_dataset("pi-camera-data-1",data=data)
                    dataset.attrs["description"] = "Radiative Heat values recorded by a Raspberry Pi Thermal Camera"
                    dataset.attrs["unitType"] = "Joules/m2"
                else:
                    dataset = pi_group["pi-camera-data-1"]
                    dataset[...] = data
                    
                 dataset.attrs["size"] = dataset.shape
                 dataset.attrs["dtype"] = str(dataset.dtype)
            # if it's a list of datasets
            elif type(data) is list:
                # iterate through list naming datasets incremenetally
                for cd,d in enumerate(data):
                    if not "pi-camera-data-{0}".format(cd) in pi_group:
                        dataset = pi_group.create_dataset("pi-camera-data-{0}".format(cd),data=d)
                        dataset.attrs["description"] = "Radiative Heat values recorded by a Raspberry Pi Thermal Camera"
                        dataset.attrs["unitType"] = "Joules/m2"
                    else:
                        dataset = pi_group["pi-camera-data-{0}"]
                        dataset[...] = d
                    
                    dataset.attrs["size"] = dataset.shape
                    dataset.attrs["dtype"] = str(dataset.dtype)
            # if it's a dictionary of datasets
            elif type(data) is dict:
                # iterate through dictionary creating datasets based off the keys
                for key,val in data.items():
                    if not key in pi_group:
                        dataset = pi_group.create_dataset(key,data=val)
                        dataset.attrs["description"] = "Radiative Heat values recorded by a Raspberry Pi Thermal Camera"
                        dataset.attrs["unitType"] = "Joules/m2"
                    else:
                        dataset = pi_group[key]
                        dataset[...] = val
                        
                    dataset.attrs["size"] = dataset.shape
                    dataset.attrs["dtype"] = str(dataset.dtype)
        # update time stamp
        run_group.attrs["Date-Time"] = datetime.datetime.now().isoformat()

        # return true and updated run group indicating success
        return True,run_group
        
def getNumRuns(h5_file):
    """ Count the number of runs currently in file

        h5_file : File object

        Iterates through the items under Runs group
        Returns 0 if no runs are found or there's no
        Runs groups

        Returns the number of found groups
    """
    # check if file does not has run group
    # if no Runs group, return 0
    if not "Runs" in h5_file:
        return 0
    else:
        # visit each item in run and check if it is a group
        # add to list if it is a group and if the group name is number
        # means it is a Run
        # Numbered runs are stored as groups
        num_keys = len([keys for keys in h5_file["Runs"].keys()])
        # return number of items in list
        return num_keys

    
def printFileStructure(item,leading='  '):
    """ Print the h5 or dictionary file structure

        Prints the keys of each item as an indented list.
        If the item is a dataset, then the shape is printed.
        Level of indentation indicates level of depth in the tree
    """
    import h5py
    # code adapted from
    #https://stackoverflow.com/questions/34330283/how-to-differentiate-between-hdf5-datasets-and-groups-with-h5py
    for key in item:
        # if object is a dataset print its name followed by shape
        if isinstance(item[key], h5py.Dataset):
            print(leading + key + ": " + str(item[key].shape))
            #get attributes
            atts = item[key].attrs.items()
            # if there are attributes
            if len(atts)>0:
                # print name and value
                # equals sign indicates its a attribute
                # prints it at a different indent level for ease of reading
                for k,v in atts:
                    print('  '+leading+k, "= ",v)
        # if it's something else
        else:
            print(leading + key)
            printFileStructure(item[key],leading=leading+'  ')

def initialize(path):
    """ Create and initialize HDf5 at the specified full file path

        path : full file path including file extension .hdf5

        Creates a new h5 file as specified in the path.
        Creates the high level group Runs

        Returns the file object for the newly created file
    """
    import h5py
    ## Create HPF5 file
    f = h5py.File("arrow-shape-data.hdf5","w")
        
    ## high level groups
    f.create_group("Runs")

    # return file object
    return f

def demo():
    import numpy as np
    import datetime as dt
    # create and initialize file
    print("Creating and initializing hdf5 file")
    f = initialize("demo-data.hdf5")
    # create a random number of test groups
    for i in range(np.random.randint(low=1,high=10,size=1)[0]):
        print("Creating group ",i+1)
        new_run = createRunGroup(f)
        
    # if failed to create run group
    if new_run == False:
        print("Failed to create group")
    # if successful
    else:
        print("Successfully created ",i+1," test run groups")

        # print tree structure to screen to show creation
        printFileStructure(f)

        # print the number of run groups added to test fn
        print("File contains ",getNumRuns(f), " groups")

        # add random data set to data
        print("Adding random Qr dataset")
        if updateData(f,Qr=np.random.rand(100))[0]:
            print("Success adding random Qr dataset")

        # read in xml file and write it into file
        print("Adding randomly generated XML dataset")
        # creating empty dummy list
        xml = [None]*7
        # initialize first element as dictionary
        xml[0] = {}
        #header,time,torque,vel,x,y,z
        # generate random header data
        # set data length as the same for all to best simulate actual data
        rand_len = np.random.randint(1e5)
        for i in range(5):
            xml[0]["data-signal-{:d}".format(i)] = {"interval": np.random.rand(1),
                                                    "description": "this is a demo",
                                                    "count":rand_len,
                                                    "unitType":"nothing"}
        # set date time flag as current time
        xml[0]["Date-Time"] = dt.datetime.now().isoformat()
        # generate random data for other datasets
        # using same length as set before
        for i in range(1,7):
            xml[i] = np.random.rand(rand_len)
        
        if updateData(f,xml=xml)[0]:
            print("Success adding XML dataset")

        # print structure to show result
        printFileStructure(f)

        # print list of dataets added to file
        # prints the name, shape and data type
        print("Getting all datasets added")
        dsets = getAllDatasets(f)
        print("\nFound ",len(dsets)," datasets")
        for key,val in dsets.items():
            print(key," : ",val.shape,val.dtype)
            # print attributes
            print("\tAttributes:")
            for akey,aval in f[key].attrs.items():
                print("\t\t",akey," : ",aval)

        print("\nAttributes for x")
        for key,val in f["Runs/{:d}/Data/Laser/x".format(getNumRuns(f))].attrs.items():
            print(key,", ",val)

        print("Getting latest run")
        lrun = getLatestRun(f)
        print("Latest run called: ",lrun.name)
        # print structure
        printFileStructure(lrun)

        print("Getting date time stamps of groups in file")
        for i in range(getNumRuns(f)):
            # retrieve run
            r = getRun(f,i+1)
            # if run was retrieved
            if r!=None:
                print("Group ",r.name," was created on",r.attrs["Date-Time"])

    f.close()

